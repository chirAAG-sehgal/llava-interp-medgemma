{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8771af45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[0.8509, 0.6640, 0.4625],\n",
      "        [0.2430, 0.5951, 0.3591]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # should be True\n",
    "print(torch.rand(2,3).to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "314a2e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiragjishu/miniconda3/envs/llava-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "import torch\n",
    "from PIL import Image\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04bfd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.43s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                    \"llava-hf/llava-1.5-7b-hf\",\n",
    "                    torch_dtype=torch.float16, \n",
    "                    low_cpu_mem_usage=True,\n",
    "                    device_map=\"auto\",\n",
    "                    cache_dir=\"/home/chiragjishu/chirag/VLMs/llava-interp/models\"\n",
    "                )\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "478c1d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "1\n",
      "tensor([[    1,  3148,  1001, 29901, 29871, 32000, 29871,    13,  4002, 29581,\n",
      "           278,  1967,   319,  1799,  9047, 13566, 29901]])\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "text_question = \"Describe the image\"\n",
    "prompt = f\"USER: <image>\\n{text_question} ASSISTANT:\"\n",
    "image = Image.open(\"/home/chiragjishu/chirag/VLMs/llava-interp/image_folder/casualty.jpg\")\n",
    "inputs = processor(text=prompt, return_tensors = \"pt\")\n",
    "image_token_id = processor.image_token_id\n",
    "\n",
    "print(image_token_id)\n",
    "sum=0\n",
    "for i in inputs['input_ids'][0]:\n",
    "    # print(i.item().type())\n",
    "    if i.item()==image_token_id:\n",
    "        sum+=1\n",
    "print(sum)\n",
    "print(inputs['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "307cadb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b1d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c40a77e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values', 'image_hidden_states'])\n"
     ]
    }
   ],
   "source": [
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc04c77",
   "metadata": {},
   "source": [
    "## checking for medgemma's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a99160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 105, 2364, 107, 3048, 659, 614, 7710, 1171, 3072, 6992, 236761, 108, 82858, 672, 2471, 108, 255999, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 256000, 108, 106, 107, 105, 4368, 107]]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/medgemma-4b-it\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "image = Image.open(\"/home/chiragjishu/chirag/VLMs/llava-interp/image_folder/casualty.jpg\") \n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are an expert first response medic.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image\"},\n",
    "            {\"type\": \"image\", \"image\": image}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,\n",
    "    return_dict=True,\n",
    ")\n",
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a060a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', '<start_of_turn>', 'user', '\\n', 'You', ' are', ' an', ' expert', ' first', ' response', ' medic', '.', '\\n\\n', 'Describe', ' this', ' image', '\\n\\n', '<start_of_image>', '<IMG001>', '<IMG002>', '<IMG003>', '<IMG004>', '<IMG005>', '<IMG006>', '<IMG007>', '<IMG008>', '<IMG009>', '<IMG010>', '<IMG011>', '<IMG012>', '<IMG013>', '<IMG014>', '<IMG015>', '<IMG016>', '<IMG017>', '<IMG018>', '<IMG019>', '<IMG020>', '<IMG021>', '<IMG022>', '<IMG023>', '<IMG024>', '<IMG025>', '<IMG026>', '<IMG027>', '<IMG028>', '<IMG029>', '<IMG030>', '<IMG031>', '<IMG032>', '<IMG033>', '<IMG034>', '<IMG035>', '<IMG036>', '<IMG037>', '<IMG038>', '<IMG039>', '<IMG040>', '<IMG041>', '<IMG042>', '<IMG043>', '<IMG044>', '<IMG045>', '<IMG046>', '<IMG047>', '<IMG048>', '<IMG049>', '<IMG050>', '<IMG051>', '<IMG052>', '<IMG053>', '<IMG054>', '<IMG055>', '<IMG056>', '<IMG057>', '<IMG058>', '<IMG059>', '<IMG060>', '<IMG061>', '<IMG062>', '<IMG063>', '<IMG064>', '<IMG065>', '<IMG066>', '<IMG067>', '<IMG068>', '<IMG069>', '<IMG070>', '<IMG071>', '<IMG072>', '<IMG073>', '<IMG074>', '<IMG075>', '<IMG076>', '<IMG077>', '<IMG078>', '<IMG079>', '<IMG080>', '<IMG081>', '<IMG082>', '<IMG083>', '<IMG084>', '<IMG085>', '<IMG086>', '<IMG087>', '<IMG088>', '<IMG089>', '<IMG090>', '<IMG091>', '<IMG092>', '<IMG093>', '<IMG094>', '<IMG095>', '<IMG096>', '<IMG097>', '<IMG098>', '<IMG099>', '<IMG100>', '<IMG101>', '<IMG102>', '<IMG103>', '<IMG104>', '<IMG105>', '<IMG106>', '<IMG107>', '<IMG108>', '<IMG109>', '<IMG110>', '<IMG111>', '<IMG112>', '<IMG113>', '<IMG114>', '<IMG115>', '<IMG116>', '<IMG117>', '<IMG118>', '<IMG119>', '<IMG120>', '<IMG121>', '<IMG122>', '<IMG123>', '<IMG124>', '<IMG125>', '<IMG126>', '<IMG127>', '<IMG128>', '<IMG129>', '<IMG130>', '<IMG131>', '<IMG132>', '<IMG133>', '<IMG134>', '<IMG135>', '<IMG136>', '<IMG137>', '<IMG138>', '<IMG139>', '<IMG140>', '<IMG141>', '<IMG142>', '<IMG143>', '<IMG144>', '<IMG145>', '<IMG146>', '<IMG147>', '<IMG148>', '<IMG149>', '<IMG150>', '<IMG151>', '<IMG152>', '<IMG153>', '<IMG154>', '<IMG155>', '<IMG156>', '<IMG157>', '<IMG158>', '<IMG159>', '<IMG160>', '<IMG161>', '<IMG162>', '<IMG163>', '<IMG164>', '<IMG165>', '<IMG166>', '<IMG167>', '<IMG168>', '<IMG169>', '<IMG170>', '<IMG171>', '<IMG172>', '<IMG173>', '<IMG174>', '<IMG175>', '<IMG176>', '<IMG177>', '<IMG178>', '<IMG179>', '<IMG180>', '<IMG181>', '<IMG182>', '<IMG183>', '<IMG184>', '<IMG185>', '<IMG186>', '<IMG187>', '<IMG188>', '<IMG189>', '<IMG190>', '<IMG191>', '<IMG192>', '<IMG193>', '<IMG194>', '<IMG195>', '<IMG196>', '<IMG197>', '<IMG198>', '<IMG199>', '<IMG200>', '<IMG201>', '<IMG202>', '<IMG203>', '<IMG204>', '<IMG205>', '<IMG206>', '<IMG207>', '<IMG208>', '<IMG209>', '<IMG210>', '<IMG211>', '<IMG212>', '<IMG213>', '<IMG214>', '<IMG215>', '<IMG216>', '<IMG217>', '<IMG218>', '<IMG219>', '<IMG220>', '<IMG221>', '<IMG222>', '<IMG223>', '<IMG224>', '<IMG225>', '<IMG226>', '<IMG227>', '<IMG228>', '<IMG229>', '<IMG230>', '<IMG231>', '<IMG232>', '<IMG233>', '<IMG234>', '<IMG235>', '<IMG236>', '<IMG237>', '<IMG238>', '<IMG239>', '<IMG240>', '<IMG241>', '<IMG242>', '<IMG243>', '<IMG244>', '<IMG245>', '<IMG246>', '<IMG247>', '<IMG248>', '<IMG249>', '<IMG250>', '<IMG251>', '<IMG252>', '<IMG253>', '<IMG254>', '<IMG255>', '<IMG256>', '<end_of_image>', '\\n\\n', '<end_of_turn>', '\\n', '<start_of_turn>', 'model', '\\n']\n"
     ]
    }
   ],
   "source": [
    "input_ids = inputs['input_ids'][0]\n",
    "img_token_id = processor.image_token_id\n",
    "token_labels = []\n",
    "# image_token_count = 256\n",
    "i = 0\n",
    "for token_id in input_ids:\n",
    "        if token_id == img_token_id:\n",
    "            # One indexed because the HTML logic wants it that way\n",
    "            # token_labels.extend([f\"<IMG{(i+1):03d}>\" for i in range(img_token_count)])\n",
    "            token_labels.append(f\"<IMG{(i+1):03d}>\")\n",
    "            i+=1\n",
    "        else:\n",
    "            token_labels.append(processor.tokenizer.decode([token_id]))\n",
    "print(token_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9508ad14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaForConditionalGeneration(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPSdpaAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (multi_modal_projector): LlavaMultiModalProjector(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELUActivation()\n",
      "    (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  )\n",
      "  (language_model): LlamaForCausalLM(\n",
      "    (model): LlamaModel(\n",
      "      (embed_tokens): Embedding(32064, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x LlamaDecoderLayer(\n",
      "          (self_attn): LlamaSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fe7c374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.type of LlamaRMSNorm((4096,), eps=1e-05)>\n"
     ]
    }
   ],
   "source": [
    "print(model.language_model.model.norm.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cced2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_locn = \"text_model_in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3c0025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiragjishu/miniconda3/envs/llava-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.41s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "model_id = \"google/medgemma-4b-it\"\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f1f8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3ForConditionalGeneration(\n",
      "  (vision_tower): SiglipVisionModel(\n",
      "    (vision_model): SiglipVisionTransformer(\n",
      "      (embeddings): SiglipVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "        (position_embedding): Embedding(4096, 1152)\n",
      "      )\n",
      "      (encoder): SiglipEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-26): 27 x SiglipEncoderLayer(\n",
      "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (self_attn): SiglipAttention(\n",
      "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): SiglipMLP(\n",
      "              (activation_fn): PytorchGELUTanh()\n",
      "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (multi_modal_projector): Gemma3MultiModalProjector(\n",
      "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "  )\n",
      "  (language_model): Gemma3ForCausalLM(\n",
      "    (model): Gemma3TextModel(\n",
      "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "      (layers): ModuleList(\n",
      "        (0-33): 34 x Gemma3DecoderLayer(\n",
      "          (self_attn): Gemma3Attention(\n",
      "            (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          )\n",
      "          (mlp): Gemma3MLP(\n",
      "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "            (act_fn): PytorchGELUTanh()\n",
      "          )\n",
      "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (rotary_emb): Gemma3RotaryEmbedding()\n",
      "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"/home/chiragjishu/chirag/VLMs/llava-interp/image_folder/casualty.jpg\") \n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are an expert first response medic.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image\"},\n",
    "            {\"type\": \"image\", \"image\": image}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,\n",
    "    return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "# inputs  = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs, output_hidden_states=True, max_new_tokens=250)\n",
    "hidden_states = output.hidden_states\n",
    "print(len(hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 281, 2560])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.image_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipVisionConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
       "  \"hidden_size\": 1152,\n",
       "  \"image_size\": 896,\n",
       "  \"intermediate_size\": 4304,\n",
       "  \"layer_norm_eps\": 1e-06,\n",
       "  \"model_type\": \"siglip_vision_model\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 27,\n",
       "  \"patch_size\": 14,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"vision_use_head\": false\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vision_tower.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'token_type_ids', 'pixel_values'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs['input_ids'][0]))\n",
    "sum=0\n",
    "for i in inputs['input_ids'][0]:\n",
    "    # print(i.item().type())\n",
    "    if i.item()==262144:\n",
    "        sum+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
